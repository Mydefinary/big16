from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import json
import os
from datetime import datetime
import uvicorn
from dotenv import load_dotenv
from pathlib import Path
import re
from collections import defaultdict, Counter
from itertools import combinations
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ (ì„ íƒì  import) - ìž„ì‹œë¡œ ë¹„í™œì„±í™”
try:
    # from konlpy.tag import Okt
    KONLPY_AVAILABLE = False
    print("Info: KoNLPy temporarily disabled for Java-free deployment.")
except ImportError:
    KONLPY_AVAILABLE = False
    print("Warning: KoNLPy not available. Using basic text processing.")

load_dotenv()

app = FastAPI(
    title="ì›¹íˆ° ë¶„ì„ API - TF-IDF Enhanced",
    description="TF-IDF ê¸°ë°˜ ì¤„ê±°ë¦¬ ë¶„ì„ì´ ì¶”ê°€ëœ ì›¹íˆ° ì¶”ì²œ ì‹œìŠ¤í…œ API",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://localhost:3002",
        "https://webtoon-analytics-dashboard.vercel.app",
        "https://webtoon-analytics-dashboard-1flmwo7bk.vercel.app",
        "http://20.249.154.2",
        "http://20.249.113.18:9000",
        "*"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=[
        "Accept",
        "Accept-Language", 
        "Content-Language",
        "Content-Type",
        "Authorization",
    ],
)

# ë°ì´í„° ëª¨ë¸
class WebtoonData(BaseModel):
    rank: int
    title: str
    summary: str
    tags: List[str]
    interest_count: int
    rating: float
    gender: str
    ages: str

class EnhancedRecommendationRequest(BaseModel):
    title: str
    limit: Optional[int] = 5
    use_tfidf: Optional[bool] = True
    tfidf_weight: Optional[float] = 0.4

class SummaryAnalysisRequest(BaseModel):
    text: str
    max_keywords: Optional[int] = 10

class TFIDFAnalysisResponse(BaseModel):
    success: bool
    data: Optional[Dict] = None
    message: Optional[str] = None

# ê¸°ì¡´ íƒœê·¸ ì •ê·œí™” ë§¤í•‘
TAG_NORMALIZATION = {
    "ì™„ê²°ë¡œë§¨ìŠ¤": "ë¡œë§¨ìŠ¤",
    "ì™„ê²° ë¡œë§¨ìŠ¤": "ë¡œë§¨ìŠ¤", 
    "ìˆœì •": "ë¡œë§¨ìŠ¤",
    "ì—°ì• ": "ë¡œë§¨ìŠ¤",
    "ëŸ¬ë¸Œ": "ë¡œë§¨ìŠ¤",
    "ì™„ê²°ì•¡ì…˜": "ì•¡ì…˜",
    "ì™„ê²° ì•¡ì…˜": "ì•¡ì…˜",
    "ë°°í‹€": "ì•¡ì…˜", 
    "ê²©íˆ¬": "ì•¡ì…˜",
    "ì „íˆ¬": "ì•¡ì…˜",
    "ì™„ê²°íŒíƒ€ì§€": "íŒíƒ€ì§€",
    "ì™„ê²° íŒíƒ€ì§€": "íŒíƒ€ì§€",
    "ë§ˆë²•": "íŒíƒ€ì§€",
    "í™˜ìƒ": "íŒíƒ€ì§€",
    "ì´ì„¸ê³„": "íŒíƒ€ì§€",
    "ì™„ê²°ë“œë¼ë§ˆ": "ë“œë¼ë§ˆ",
    "ì™„ê²° ë“œë¼ë§ˆ": "ë“œë¼ë§ˆ",
    "ë©œë¡œ": "ë“œë¼ë§ˆ",
    "ê°ë™": "ë“œë¼ë§ˆ",
    "ì™„ê²°ìŠ¤ë¦´ëŸ¬": "ìŠ¤ë¦´ëŸ¬",
    "ì™„ê²° ìŠ¤ë¦´ëŸ¬": "ìŠ¤ë¦´ëŸ¬",
    "ì„œìŠ¤íŽœìŠ¤": "ìŠ¤ë¦´ëŸ¬",
    "ë¯¸ìŠ¤í„°ë¦¬": "ìŠ¤ë¦´ëŸ¬",
    "ì™„ê²°ì¼ìƒ": "ì¼ìƒ",
    "ì™„ê²° ì¼ìƒ": "ì¼ìƒ",
    "ížë§": "ì¼ìƒ",
    "ì†Œì†Œí•œ": "ì¼ìƒ",
    "ì„±ìž¥ë¬¼": "ì„±ìž¥",
    "ë ˆë²¨ì—…": "ì„±ìž¥",
    "ì½”ë¯¸ë””": "ê°œê·¸",
    "ì™„ê²° ê°œê·¸": "ê°œê·¸",
    "ì™„ê²°ê°œê·¸": "ê°œê·¸",
    "ì™•ì¡±/ê·€ì¡±": "ê·€ì¡±",
    "ëŸ¬ë¸”ë¦¬": "ì¼ìƒ",
    "ëª…ìž‘": "ëª…ìž‘",
    "ì™„ê²°ë¬´ë£Œ":"ê¸°íƒ€",
}

# TF-IDF ë¶„ì„ í´ëž˜ìŠ¤
class KoreanTFIDFAnalyzer:
    def __init__(self):
        self.vectorizer = None
        self.tfidf_matrix = None
        self.feature_names = None
        # KoNLPy ì‚¬ìš© ì‹œë„ (Java í•„ìš”)
        try:
            self.okt = Okt() if KONLPY_AVAILABLE else None
        except Exception as e:
            print(f"Warning: Failed to initialize KoNLPy Okt: {e}")
            self.okt = None
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì›¹íˆ° ë„ë©”ì¸ íŠ¹í™”)
        self.korean_stopwords = {
            # ê¸°ë³¸ ë¶ˆìš©ì–´
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ëŠ”', 'ì€', 'ê³¼', 'ì™€', 'ë„', 'ë§Œ', 'ë¡œ', 'ìœ¼ë¡œ',
            'ì´ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìžˆë‹¤', 'ì—†ë‹¤', 'ê·¸', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ëž˜ì„œ',
            'ë˜í•œ', 'ë˜', 'ì—­ì‹œ', 'ë¬¼ë¡ ', 'ë§Œì•½', 'ë¹„ë¡', 'ì•„ì§', 'ì´ë¯¸', 'í•­ìƒ', 'ê°€ìž¥', 'ë§¤ìš°',
            'ì •ë§', 'ì•„ì£¼', 'ë„ˆë¬´', 'ì¡°ê¸ˆ', 'ì¢€', 'ë§Žì´', 'ìž˜', 'ëª»', 'ì•ˆ', 'ë§ˆë‹¤', 'ëª¨ë“ ', 'ê°',
            'ìˆ˜', 'ê²ƒ', 'ë•Œ', 'ê³³', 'ì ', 'ë©´', 'ì¤‘', 'í›„', 'ì „', 'ë™ì•ˆ', 'ì‚¬ì´', 'ë’¤', 'ì•ž',
            # ì›¹íˆ°/ìŠ¤í† ë¦¬ ê´€ë ¨ ì¼ë°˜ì  ë¶ˆìš©ì–´
            'ì¤„ê±°ë¦¬', 'ì´ì•¼ê¸°', 'ë‚´ìš©', 'ìž‘í’ˆ', 'ì›¹íˆ°', 'ë§Œí™”', 'ì†Œì„¤', 'ë“œë¼ë§ˆ', 'ì˜í™”',
            'ì£¼ì¸ê³µ', 'ë“±ìž¥ì¸ë¬¼', 'ìºë¦­í„°', 'ì¸ë¬¼', 'ì‚¬ëžŒ', 'ë‚¨ìž', 'ì—¬ìž', 'ì†Œë…„', 'ì†Œë…€',
            'ìƒí™©', 'ê²½ìš°', 'ì‹œê°„', 'ìˆœê°„', 'í•˜ë£¨', 'ì–´ëŠë‚ ', 'ê·¸ë‚ ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
            'ì„¸ìƒ', 'ì„¸ê³„', 'í˜„ì‹¤', 'ì¼ìƒ', 'ìƒí™œ', 'ì¸ìƒ', 'ì‚¬ëž‘', 'ê´€ê³„', 'ì¹œêµ¬', 'ê°€ì¡±',
            'ë¬¸ì œ', 'ì¼', 'ìƒíƒœ', 'ë§ˆìŒ', 'ìƒê°', 'ëŠë‚Œ', 'ê¸°ë¶„', 'ê°ì •', 'í‘œí˜„', 'ë§',
            'ê³¼ì—°', 'ì •ë§ë¡œ', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì—¬ì „ížˆ', 'ë“œë””ì–´', 'ê²°êµ­', 'ë§ˆì¹¨ë‚´',
            'ë•Œë¡œëŠ”', 'ê°€ë”', 'ìžì£¼', 'ëŠ˜', 'ì–¸ì œë‚˜', 'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë', 'ì‹œìž‘',
            'ë”ìš±','ë”í•´ì ¸','ìš”ì†Œê°€','ë”í•´ì ¸','ìš”ì†Œê°€ ë”í•´ì ¸','í¥ë¯¸ë¡­ë‹¤','ë”ìš± í¥ë¯¸ë¡­ë‹¤','ëœë‹¤','ì—†ëŠ”','ìžˆì„ê¹Œ','ìžˆëŠ”',
            'ì‹œìž‘ëœë‹¤','ë˜ëŠ”ë°','ìžì‹ ì˜','ì–´ëŠ','í•¨ê»˜'
        }
    
    def preprocess_korean_text(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text or pd.isna(text):
            return ""
            
        # ê¸°ë³¸ ì •ë¦¬
        text = str(text).strip()
        text = re.sub(r'[^\w\sê°€-íž£]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ìž ì œê±° (í•œê¸€ ë³´ì¡´)
        text = re.sub(r'\s+', ' ', text)  # ì¤‘ë³µ ê³µë°± ì œê±°
        
        if self.okt:
            # KoNLPyë¥¼ ì‚¬ìš©í•œ í˜•íƒœì†Œ ë¶„ì„
            try:
                morphs = self.okt.morphs(text, stem=True)
                # ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ìž ì´ìƒ ë‹¨ì–´ë§Œ ì„ íƒ
                filtered_words = [word for word in morphs 
                                if len(word) >= 2 and word not in self.korean_stopwords]
                return ' '.join(filtered_words)
            except Exception as e:
                print(f"KoNLPy ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                return text
        else:
            # ê¸°ë³¸ ì²˜ë¦¬: ë¶ˆìš©ì–´ë§Œ ì œê±°
            words = text.split()
            filtered_words = [word for word in words 
                            if len(word) >= 2 and word not in self.korean_stopwords]
            return ' '.join(filtered_words)
    
    def fit_transform(self, texts):
        """TF-IDF ë²¡í„°í™” ìˆ˜í–‰"""
        if not texts:
            return None, None
            
        # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        processed_texts = [self.preprocess_korean_text(text) for text in texts]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        processed_texts = [text if text.strip() else "ë¹ˆë¬¸ì„œ" for text in processed_texts]
        
        # TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì •
        self.vectorizer = TfidfVectorizer(
            max_features=1000,  # ìµœëŒ€ 1000ê°œ íŠ¹ì„±
            min_df=2,  # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë“±ìž¥
            max_df=0.8,  # ì „ì²´ ë¬¸ì„œì˜ 80% ì´ìƒì—ì„œ ë“±ìž¥í•˜ëŠ” ë‹¨ì–´ ì œì™¸
            ngram_range=(1, 2),  # 1-gramê³¼ 2-gram ì‚¬ìš©
            token_pattern=r'[ê°€-íž£]{2,}|[a-zA-Z]{2,}',  # í•œê¸€ ë˜ëŠ” ì˜ì–´ 2ê¸€ìž ì´ìƒ
        )
        
        try:
            # TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            self.tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
            self.feature_names = self.vectorizer.get_feature_names_out()
            
            print(f"âœ… TF-IDF ë¶„ì„ ì™„ë£Œ - ë¬¸ì„œ ìˆ˜: {len(texts)}, íŠ¹ì„± ìˆ˜: {len(self.feature_names)}")
            return self.tfidf_matrix, self.feature_names
            
        except Exception as e:
            print(f"âŒ TF-IDF ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None, None
    
    def get_top_keywords(self, doc_index, top_k=10):
        """íŠ¹ì • ë¬¸ì„œì˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if self.tfidf_matrix is None or doc_index >= self.tfidf_matrix.shape[0]:
            return []
            
        # í•´ë‹¹ ë¬¸ì„œì˜ TF-IDF ì ìˆ˜
        doc_tfidf = self.tfidf_matrix[doc_index].toarray()[0]
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì¸ë±ìŠ¤
        top_indices = doc_tfidf.argsort()[-top_k:][::-1]
        
        # í‚¤ì›Œë“œì™€ ì ìˆ˜ ë°˜í™˜
        keywords = []
        for idx in top_indices:
            if doc_tfidf[idx] > 0:
                keywords.append({
                    'keyword': self.feature_names[idx],
                    'score': float(doc_tfidf[idx]),
                    'rank': len(keywords) + 1
                })
        
        return keywords
    
    def get_document_similarity(self, doc1_idx, doc2_idx):
        """ë‘ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.tfidf_matrix is None:
            return 0.0
            
        doc1_vector = self.tfidf_matrix[doc1_idx:doc1_idx+1]
        doc2_vector = self.tfidf_matrix[doc2_idx:doc2_idx+1]
        
        similarity = cosine_similarity(doc1_vector, doc2_vector)[0][0]
        return float(similarity)

# ê¸€ë¡œë²Œ TF-IDF ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
tfidf_analyzer = KoreanTFIDFAnalyzer()

def normalize_tag(tag):
    """íƒœê·¸ ì •ê·œí™”"""
    if not tag:
        return tag
    
    tag = tag.strip()
    normalized = TAG_NORMALIZATION.get(tag, tag)
    return normalized

def normalize_tags_in_data(webtoons_data):
    """ë°ì´í„°ì˜ ëª¨ë“  íƒœê·¸ ì •ê·œí™”"""
    for webtoon in webtoons_data:
        webtoon['tags'] = [normalize_tag(tag) for tag in webtoon['tags']]
        webtoon['normalized_tags'] = list(set(webtoon['tags']))
    return webtoons_data

# ìƒ˜í”Œ ë°ì´í„° (ì¤„ê±°ë¦¬ í¬í•¨)
SAMPLE_WEBTOONS = [
    {
        "rank": 1, "title": "í™”ì‚°ê·€í™˜", 
        "summary": "ëŒ€ í™”ì‚°íŒŒ 13ëŒ€ ì œìž. ì²œí•˜ì‚¼ëŒ€ê²€ìˆ˜ ë§¤í™”ê²€ì¡´ ì²­ëª…. ì²œí•˜ë¥¼ í˜¼ëž€ì— ë¹ ëœ¨ë¦° ê³ ê¸ˆì œì¼ë§ˆ ì²œë§ˆì˜ ëª©ì„ ì¹˜ê³  ì‹­ë§ŒëŒ€ì‚°ì˜ ì •ìƒì—ì„œ ì˜ë©´. ë°± ë…„ì˜ ì‹œê°„ì„ ë›°ì–´ë„˜ì–´ ì•„ì´ì˜ ëª¸ìœ¼ë¡œ ë‹¤ì‹œ ì‚´ì•„ë‚˜ë‹¤.",
        "tags": ["íšŒê·€", "ë¬´í˜‘", "ì•¡ì…˜", "ëª…ìž‘"], 
        "interest_count": 1534623, "rating": 9.88, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"
    },
    {
        "rank": 2, "title": "ì‹ ì˜ íƒ‘", 
        "summary": "ì‹ ì˜ íƒ‘ ê¼­ëŒ€ê¸°ì—ëŠ” ëª¨ë“  ê²ƒì´ ìžˆë‹¤ê³  í•œë‹¤. íƒ‘ì— ë“¤ì–´ê°€ ì‹œí—˜ì„ í†µê³¼í•˜ë©´ì„œ ìœ„ë¡œ ì˜¬ë¼ê°€ëŠ” ì´ì•¼ê¸°. ê° ì¸µë§ˆë‹¤ ë‹¤ë¥¸ ì‹œí—˜ê³¼ ê°•ë ¥í•œ ì ë“¤ì´ ê¸°ë‹¤ë¦¬ê³  ìžˆë‹¤.",
        "tags": ["íŒíƒ€ì§€", "ì•¡ì…˜", "ì„±ìž¥"], 
        "interest_count": 1910544, "rating": 9.84, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"
    },
    {
        "rank": 3, "title": "ì™¸ëª¨ì§€ìƒì£¼ì˜", 
        "summary": "ëª»ìƒê¸´ ì™¸ëª¨ ë•Œë¬¸ì— ê´´ë¡­íž˜ì„ ë‹¹í•˜ë˜ ì£¼ì¸ê³µì´ ì–´ëŠ ë‚  ìž˜ìƒê¸´ ëª¸ìœ¼ë¡œ ë°”ë€Œë©´ì„œ ê²ªëŠ” ì´ì•¼ê¸°. ì™¸ëª¨ì— ë”°ë¥¸ ì°¨ë³„ê³¼ ì‚¬íšŒ ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.",
        "tags": ["ë“œë¼ë§ˆ", "í•™ì›", "ì•¡ì…˜"], 
        "interest_count": 824399, "rating": 9.40, "gender": "ë‚¨ì„±", "ages": "10ëŒ€"
    },
    {
        "rank": 4, "title": "ë§ˆë¥¸ ê°€ì§€ì— ë°”ëžŒì²˜ëŸ¼", 
        "summary": "ê°€ë‚œí•œ ë°±ìž‘ ê°€ë¬¸ì˜ ë”¸ì´ ì •ëžµê²°í˜¼ì„ í†µí•´ ê³µìž‘ê°€ë¡œ ì‹œì§‘ê°€ë©´ì„œ íŽ¼ì³ì§€ëŠ” ë¡œë§¨ìŠ¤. ëƒ‰ì •í•œ ê³µìž‘ê³¼ ë”°ëœ»í•œ ë§ˆìŒì„ ê°€ì§„ ì—¬ì£¼ì¸ê³µì˜ ì‚¬ëž‘ ì´ì•¼ê¸°.",
        "tags": ["ë¡œë§¨ìŠ¤", "ê·€ì¡±", "ì„œì–‘"], 
        "interest_count": 458809, "rating": 9.97, "gender": "ì—¬ì„±", "ages": "10ëŒ€"
    },
    {
        "rank": 5, "title": "ì—„ë§ˆë¥¼ ë§Œë‚˜ëŸ¬ ê°€ëŠ” ê¸¸", 
        "summary": "íê°€ì—ì„œ ë°œê²¬ëœ ì•„ì´ ëª¨ë¦¬ëŠ” êµ¬ì¡°ëŒ€ì— ì˜í•´ ë³´í˜¸ì†Œì—ì„œ ëˆˆì„ ëœ¬ë‹¤. í›„ì›ìžì—ê²Œ ì¡°ê±´ ì—†ëŠ” ì‚¬ëž‘ì„ ë°›ê³  ìžë¼ë©´ì„œ ì—„ë§ˆë¼ëŠ” ì¡´ìž¬ë¥¼ ì•Œê²Œ ë˜ê³  ì—„ë§ˆë¥¼ ì°¾ì•„ ë– ë‚˜ëŠ” ëª¨í—˜.",
        "tags": ["íŒíƒ€ì§€", "ëª¨í—˜", "ì¼ìƒ"], 
        "interest_count": 259146, "rating": 9.98, "gender": "ì—¬ì„±", "ages": "10ëŒ€"
    },
    {
        "rank": 6, "title": "ìž¬í˜¼ í™©í›„", 
        "summary": "ì™„ë²½í•œ í™©í›„ì˜€ë˜ ë‚˜ë¹„ì—ëŠ” í™©ì œì˜ ì¼ë°©ì ì¸ ì´í˜¼ í†µë³´ë¥¼ ë°›ëŠ”ë‹¤. í•˜ì§€ë§Œ ê·¸ë…€ì—ê²ŒëŠ” ì´ë¯¸ ìƒˆë¡œìš´ ê³„íšì´ ìžˆì—ˆë‹¤. ì´ì›ƒ ë‚˜ë¼ í™©ì œì™€ì˜ ìž¬í˜¼ì„ í†µí•œ ë³µìˆ˜.",
        "tags": ["ë¡œë§¨ìŠ¤", "ê·€ì¡±", "ì„œì–‘", "ë³µìˆ˜"], 
        "interest_count": 892456, "rating": 9.75, "gender": "ì—¬ì„±", "ages": "20ëŒ€"
    },
    {
        "rank": 7, "title": "ë‚˜ í˜¼ìžë§Œ ë ˆë²¨ì—…", 
        "summary": "ì„¸ê³„ì— ë˜ì „ê³¼ í—Œí„°ê°€ ë‚˜íƒ€ë‚œ ì§€ 10ì—¬ ë…„. ì„±ì§„ìš°ëŠ” Eê¸‰ í—Œí„°ë‹¤. ì–´ëŠ ë‚  ì´ì¤‘ ë˜ì „ì—ì„œ ì£½ì„ ë»”í•œ ìˆœê°„, ì‹œìŠ¤í…œì´ ë‚˜íƒ€ë‚˜ë©° ë ˆë²¨ì—…ì„ í•  ìˆ˜ ìžˆê²Œ ëœë‹¤.",
        "tags": ["ì•¡ì…˜", "ê²Œìž„", "íŒíƒ€ì§€", "ì„±ìž¥"], 
        "interest_count": 2156789, "rating": 9.91, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"
    },
    {
        "rank": 8, "title": "ì—¬ì‹ ê°•ë¦¼", 
        "summary": "í™”ìž¥ìœ¼ë¡œ ì™„ì „ížˆ ë‹¤ë¥¸ ì‚¬ëžŒì´ ëœ ì£¼ì¸ê³µì˜ í•™ì› ë¡œë§¨ìŠ¤. ì§„ì§œ ì–¼êµ´ì„ ìˆ¨ê¸´ ì±„ ì¸ê¸°ë¥¼ ì–»ì§€ë§Œ, ì§„ì‹¤ì´ ë°í˜€ì§ˆê¹Œ ë‘ë ¤ì›Œí•œë‹¤.",
        "tags": ["ë¡œë§¨ìŠ¤", "í•™ì›", "ì¼ìƒ", "ì½”ë¯¸ë””"], 
        "interest_count": 1345678, "rating": 9.62, "gender": "ì—¬ì„±", "ages": "10ëŒ€"
    },
    {
        "rank": 9, "title": "ì´íƒœì› í´ë¼ì“°", 
        "summary": "ì•„ë²„ì§€ì˜ ì£½ìŒ ì´í›„ ë³µìˆ˜ë¥¼ ë‹¤ì§í•œ ì£¼ì¸ê³µì´ ì´íƒœì›ì—ì„œ ìž‘ì€ ìˆ ì§‘ì„ ì‹œìž‘ìœ¼ë¡œ ëŒ€ê¸°ì—…ì— ë§žì„œëŠ” ì„±ìž¥ ìŠ¤í† ë¦¬. í˜„ì‹¤ì ì¸ ì‚¬íšŒ ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.",
        "tags": ["ë“œë¼ë§ˆ", "í˜„ì‹¤", "ì„±ìž¥"], 
        "interest_count": 987654, "rating": 9.55, "gender": "ë‚¨ì„±", "ages": "30ëŒ€"
    },
    {
        "rank": 10, "title": "ìœ ë¯¸ì˜ ì„¸í¬ë“¤", 
        "summary": "í‰ë²”í•œ ì§ìž¥ì¸ ìœ ë¯¸ì˜ ë¨¸ë¦¿ì† ì„¸í¬ë“¤ì´ ë²Œì´ëŠ” ì´ì•¼ê¸°. ì—°ì• , ì§ìž¥, ì¼ìƒì˜ ê³ ë¯¼ì„ ì„¸í¬ë“¤ì˜ ì‹œì ì—ì„œ ìœ ì¾Œí•˜ê²Œ ê·¸ë ¤ë‚¸ë‹¤.",
        "tags": ["ë¡œë§¨ìŠ¤", "ì¼ìƒ", "ë“œë¼ë§ˆ"], 
        "interest_count": 756432, "rating": 9.33, "gender": "ì—¬ì„±", "ages": "30ëŒ€"
    },
]

def parse_tags(tags_str):
    """íƒœê·¸ ë¬¸ìžì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
    if pd.isna(tags_str):
        return []
    
    tags_str = str(tags_str).strip()
    
    if tags_str.startswith('[') and tags_str.endswith(']'):
        try:
            import ast
            return [normalize_tag(tag) for tag in ast.literal_eval(tags_str)]
        except:
            tags_str = tags_str[1:-1]
    
    tags = []
    for tag in tags_str.split(','):
        tag = tag.strip().strip("'\"")
        if tag:
            tags.append(normalize_tag(tag))
    
    return tags

def load_webtoon_data_from_csv_safe():
    """ì•ˆì „í•œ CSV ë°ì´í„° ë¡œë“œ"""
    try:
        csv_path = Path(__file__).parent / "final_webtoon_clean.csv"
        
        if not csv_path.exists():
            print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return normalize_tags_in_data(SAMPLE_WEBTOONS)
        
        df = pd.read_csv(csv_path)
        print(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œ í–‰ì„ ì½ì—ˆìŠµë‹ˆë‹¤.")
        
        webtoons_data = []
        for idx, row in df.iterrows():
            try:
                webtoon = {
                    "rank": int(row['rank']),
                    "title": str(row['title']),
                    "summary": str(row.get('summary', '')),
                    "tags": parse_tags(row['tags']),
                    "interest_count": int(row['interest_count']),
                    "rating": float(row['rating']),
                    "gender": str(row['gender']),
                    "ages": str(row['ages'])
                }
                webtoons_data.append(webtoon)
            except Exception as e:
                print(f"í–‰ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ì„±ê³µì ìœ¼ë¡œ {len(webtoons_data)}ê°œì˜ ì›¹íˆ° ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return normalize_tags_in_data(webtoons_data)
        
    except Exception as e:
        print(f"CSV ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return normalize_tags_in_data(SAMPLE_WEBTOONS)

def load_webtoon_data():
    """ì›¹íˆ° ë°ì´í„° ë¡œë“œ ë° TF-IDF ë¶„ì„ ìˆ˜í–‰"""
    webtoons_data = load_webtoon_data_from_csv_safe()
    
    # TF-IDF ë¶„ì„ ìˆ˜í–‰
    summaries = [w['summary'] for w in webtoons_data]
    tfidf_matrix, feature_names = tfidf_analyzer.fit_transform(summaries)
    
    if tfidf_matrix is not None:
        print(f"âœ… TF-IDF ë¶„ì„ ì™„ë£Œ: {len(webtoons_data)}ê°œ ì›¹íˆ°, {len(feature_names)}ê°œ íŠ¹ì„±")
    else:
        print("âŒ TF-IDF ë¶„ì„ ì‹¤íŒ¨")
    
    return webtoons_data

def calculate_enhanced_similarity(webtoon1_idx, webtoon2_idx, webtoons_data, tfidf_weight=0.4):
    """íƒœê·¸ + TF-IDF ê¸°ë°˜ í–¥ìƒëœ ìœ ì‚¬ë„ ê³„ì‚°"""
    w1 = webtoons_data[webtoon1_idx]
    w2 = webtoons_data[webtoon2_idx]
    
    # 1. íƒœê·¸ ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
    tags1 = set(w1['tags'])
    tags2 = set(w2['tags'])
    
    intersection = len(tags1 & tags2)
    union = len(tags1 | tags2)
    jaccard_similarity = intersection / union if union > 0 else 0
    
    # 2. TF-IDF ê¸°ë°˜ ì¤„ê±°ë¦¬ ìœ ì‚¬ë„
    tfidf_similarity = 0
    if tfidf_analyzer.tfidf_matrix is not None:
        tfidf_similarity = tfidf_analyzer.get_document_similarity(webtoon1_idx, webtoon2_idx)
    
    # 3. í‰ì /ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜
    rating_similarity = 1 - abs(w1['rating'] - w2['rating']) / 10
    popularity_factor = min(w2['interest_count'] / 1000000, 1.0)
    
    # 4. ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
    tag_weight = 1 - tfidf_weight
    final_similarity = (
        jaccard_similarity * tag_weight * 0.7 +
        tfidf_similarity * tfidf_weight +
        rating_similarity * 0.15 +
        popularity_factor * 0.15
    )
    
    return {
        'final_similarity': final_similarity,
        'jaccard_similarity': jaccard_similarity,
        'tfidf_similarity': tfidf_similarity,
        'rating_similarity': rating_similarity,
        'common_tags': list(tags1 & tags2)
    }

# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/webtoon-api/api/")
async def read_root():
    return {
        "message": "TF-IDF ê¸°ë°˜ ì›¹íˆ° ë¶„ì„ API ì„œë²„ê°€ ì •ìƒ ìž‘ë™ ì¤‘ìž…ë‹ˆë‹¤",
        "version": "2.0.0",
        "features": [
            "TF-IDF ì¤„ê±°ë¦¬ ë¶„ì„", 
            "í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ", 
            "í‚¤ì›Œë“œ ìžë™ ì¶”ì¶œ",
            "í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬"
        ],
        "endpoints": {
            "webtoons": "/webtoon-api/api/webtoons",
            "tfidf_analysis": "/webtoon-api/api/analysis/tfidf", 
            "summary_keywords": "/webtoon-api/api/analysis/summary-keywords",
            "enhanced_recommendations": "/webtoon-api/api/recommendations/enhanced",
            "similarity_analysis": "/webtoon-api/api/analysis/similarity",
            "related_tags": "/webtoon-api/api/analysis/related-tags/{tag}",
            "network_analysis": "/webtoon-api/api/analysis/network",
            "tag_connectivity": "/webtoon-api/api/analysis/tag-connectivity",
            "insights": "/webtoon-api/api/analysis/insights",
            "stats": "/webtoon-api/api/stats",
            "heatmap": "/webtoon-api/api/analysis/heatmap"
        }
    }

@app.get("/webtoon-api/api/webtoons")
async def get_webtoons():
    """ëª¨ë“  ì›¹íˆ° ë°ì´í„° ë°˜í™˜"""
    try:
        data = load_webtoon_data()
        return {"success": True, "data": data, "count": len(data)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/analysis/tfidf")
async def get_tfidf_analysis():
    """TF-IDF ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
    try:
        webtoons_data = load_webtoon_data()
        
        if tfidf_analyzer.tfidf_matrix is None:
            return {"success": False, "message": "TF-IDF ë¶„ì„ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        feature_names = tfidf_analyzer.feature_names
        tfidf_matrix = tfidf_analyzer.tfidf_matrix
        
        # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
        top_indices = mean_scores.argsort()[-20:][::-1]
        
        global_keywords = []
        for idx in top_indices:
            if mean_scores[idx] > 0:
                global_keywords.append({
                    'keyword': feature_names[idx],
                    'avg_score': float(mean_scores[idx]),
                    'rank': len(global_keywords) + 1
                })
        
        # ê° ì›¹íˆ°ë³„ ìƒìœ„ í‚¤ì›Œë“œ (ìƒ˜í”Œ)
        webtoon_keywords = {}
        for i in range(min(5, len(webtoons_data))):  # ìƒìœ„ 5ê°œë§Œ
            keywords = tfidf_analyzer.get_top_keywords(i, top_k=5)
            webtoon_keywords[webtoons_data[i]['title']] = keywords
        
        return {
            "success": True,
            "data": {
                "global_keywords": global_keywords,
                "webtoon_keywords": webtoon_keywords,
                "total_features": len(feature_names),
                "total_documents": len(webtoons_data),
                "analysis_method": "TF-IDF with Korean preprocessing"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"TF-IDF ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/webtoon-api/api/analysis/summary-keywords")
async def extract_summary_keywords(request: SummaryAnalysisRequest):
    """ì¤„ê±°ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if not request.text.strip():
            return {"success": False, "message": "ë¹ˆ í…ìŠ¤íŠ¸ìž…ë‹ˆë‹¤"}
        
        # ìž„ì‹œ TF-IDF ë¶„ì„
        temp_analyzer = KoreanTFIDFAnalyzer()
        processed_text = temp_analyzer.preprocess_korean_text(request.text)
        
        # ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„ì„ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ì™€ í•¨ê»˜ ë¶„ì„
        webtoons_data = load_webtoon_data()
        all_summaries = [w['summary'] for w in webtoons_data] + [request.text]
        
        tfidf_matrix, feature_names = temp_analyzer.fit_transform(all_summaries)
        
        if tfidf_matrix is None:
            return {"success": False, "message": "TF-IDF ë¶„ì„ ì‹¤íŒ¨"}
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ(ìž…ë ¥ í…ìŠ¤íŠ¸)ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = temp_analyzer.get_top_keywords(len(all_summaries) - 1, request.max_keywords)
        
        return {
            "success": True,
            "data": {
                "original_text": request.text,
                "processed_text": processed_text,
                "keywords": keywords,
                "keyword_count": len(keywords)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

@app.post("/webtoon-api/api/recommendations")
async def get_recommendations(request: EnhancedRecommendationRequest):
    """ê¸°ë³¸ ì›¹íˆ° ì¶”ì²œ API"""
    return await get_enhanced_recommendations(request)

@app.post("/webtoon-api/api/recommendations/enhanced") 
async def get_enhanced_recommendations(request: EnhancedRecommendationRequest):
    """TF-IDF ê¸°ë°˜ í–¥ìƒëœ ì›¹íˆ° ì¶”ì²œ"""
    try:
        webtoons_data = load_webtoon_data()
        
        target_webtoon = next((w for i, w in enumerate(webtoons_data) 
                              if w['title'] == request.title), None)
        if not target_webtoon:
            return {"success": False, "message": "ì›¹íˆ°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        target_idx = next(i for i, w in enumerate(webtoons_data) 
                         if w['title'] == request.title)
        
        recommendations = []
        
        for i, webtoon in enumerate(webtoons_data):
            if webtoon['title'] == request.title:
                continue
            
            if request.use_tfidf and tfidf_analyzer.tfidf_matrix is not None:
                # TF-IDF í¬í•¨ í–¥ìƒëœ ìœ ì‚¬ë„
                similarity_data = calculate_enhanced_similarity(
                    target_idx, i, webtoons_data, request.tfidf_weight
                )
                
                # ì¤„ê±°ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                target_keywords = tfidf_analyzer.get_top_keywords(target_idx, 5)
                candidate_keywords = tfidf_analyzer.get_top_keywords(i, 5)
                
                recommendations.append({
                    **webtoon,
                    'similarity': similarity_data['final_similarity'],
                    'jaccard_similarity': similarity_data['jaccard_similarity'],
                    'tfidf_similarity': similarity_data['tfidf_similarity'], 
                    'rating_similarity': similarity_data['rating_similarity'],
                    'common_tags': similarity_data['common_tags'],
                    'target_keywords': [kw['keyword'] for kw in target_keywords],
                    'candidate_keywords': [kw['keyword'] for kw in candidate_keywords],
                    'analysis_method': 'hybrid_tfidf_tags'
                })
            else:
                # ê¸°ì¡´ íƒœê·¸ ê¸°ë°˜ ìœ ì‚¬ë„ë§Œ
                target_tags = set(target_webtoon['tags'])
                webtoon_tags = set(webtoon['tags'])
                
                intersection = len(target_tags & webtoon_tags)
                union = len(target_tags | webtoon_tags)
                jaccard_similarity = intersection / union if union > 0 else 0
                
                rating_weight = webtoon['rating'] / 10.0
                popularity_weight = min(webtoon['interest_count'] / 1000000, 1.0)
                
                final_similarity = jaccard_similarity * 0.7 + rating_weight * 0.2 + popularity_weight * 0.1
                
                recommendations.append({
                    **webtoon,
                    'similarity': final_similarity,
                    'jaccard_similarity': jaccard_similarity,
                    'tfidf_similarity': 0,
                    'common_tags': list(target_tags & webtoon_tags),
                    'analysis_method': 'tags_only'
                })
        
        recommendations.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            "success": True,
            "data": recommendations[:request.limit],
            "count": len(recommendations),
            "requested_title": request.title,
            "target_tags": target_webtoon['tags'],
            "algorithm": "enhanced_tfidf_hybrid" if request.use_tfidf else "traditional_tags",
            "tfidf_enabled": request.use_tfidf,
            "tfidf_weight": request.tfidf_weight if request.use_tfidf else 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í–¥ìƒëœ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/analysis/similarity/{title1}/{title2}")
async def get_similarity_analysis(title1: str, title2: str):
    """ë‘ ì›¹íˆ° ê°„ ìƒì„¸ ìœ ì‚¬ë„ ë¶„ì„"""
    try:
        webtoons_data = load_webtoon_data()
        
        webtoon1_idx = next((i for i, w in enumerate(webtoons_data) if w['title'] == title1), None)
        webtoon2_idx = next((i for i, w in enumerate(webtoons_data) if w['title'] == title2), None)
        
        if webtoon1_idx is None or webtoon2_idx is None:
            return {"success": False, "message": "ì›¹íˆ°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        webtoon1 = webtoons_data[webtoon1_idx]
        webtoon2 = webtoons_data[webtoon2_idx]
        
        # ìƒì„¸ ìœ ì‚¬ë„ ë¶„ì„
        similarity_data = calculate_enhanced_similarity(webtoon1_idx, webtoon2_idx, webtoons_data)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords1 = tfidf_analyzer.get_top_keywords(webtoon1_idx, 10) if tfidf_analyzer.tfidf_matrix is not None else []
        keywords2 = tfidf_analyzer.get_top_keywords(webtoon2_idx, 10) if tfidf_analyzer.tfidf_matrix is not None else []
        
        return {
            "success": True,
            "data": {
                "webtoon1": {
                    "title": webtoon1['title'],
                    "summary": webtoon1['summary'],
                    "tags": webtoon1['tags'],
                    "keywords": keywords1
                },
                "webtoon2": {
                    "title": webtoon2['title'], 
                    "summary": webtoon2['summary'],
                    "tags": webtoon2['tags'],
                    "keywords": keywords2
                },
                "similarity_analysis": similarity_data,
                "comparison": {
                    "common_tags": similarity_data['common_tags'],
                    "common_keywords": [kw for kw in [k['keyword'] for k in keywords1] 
                                      if kw in [k['keyword'] for k in keywords2]],
                    "rating_difference": abs(webtoon1['rating'] - webtoon2['rating']),
                    "popularity_ratio": webtoon2['interest_count'] / max(webtoon1['interest_count'], 1)
                }
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/analysis/tags")
async def get_tag_analysis():
    """íƒœê·¸ ë¶„ì„ ë°ì´í„° ë°˜í™˜ (í•œêµ­ì–´ ê¸°ë°˜)"""
    try:
        webtoons_data = load_webtoon_data()
        
        # ëª¨ë“  íƒœê·¸ ìˆ˜ì§‘
        all_tags = []
        for webtoon in webtoons_data:
            all_tags.extend(webtoon['tags'])
        
        tag_frequency = Counter(all_tags).most_common(20)
        
        return {
            "success": True,
            "data": {
                "tag_frequency": tag_frequency,
                "total_tags": len(set(all_tags)),
                "normalization_applied": True,
                "korean_support": True
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒœê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/api/analysis/network")
async def get_network_analysis(
    selected_tags: Optional[str] = Query(None, description="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„ íƒëœ íƒœê·¸ë“¤"),
    min_correlation: Optional[float] = Query(0.2, description="ìµœì†Œ ìƒê´€ê³„ìˆ˜"),
    max_nodes: Optional[int] = Query(30, description="ìµœëŒ€ ë…¸ë“œ ìˆ˜")
):
    """ê³ ê¸‰ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë°ì´í„° ë°˜í™˜"""
    try:
        webtoons_data = load_webtoon_data()
        
        # ì„ íƒëœ íƒœê·¸ íŒŒì‹±
        selected_tag_list = []
        if selected_tags:
            selected_tag_list = [tag.strip() for tag in selected_tags.split(',') if tag.strip()]
        
        network_data = create_advanced_network_data(
            webtoons_data, 
            selected_tag_list, 
            min_correlation, 
            max_nodes
        )
        
        return {"success": True, "data": network_data}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/analysis/related-tags/{tag}")
async def get_related_tags_analysis(tag: str, limit: Optional[int] = Query(10)):
    """íŠ¹ì • íƒœê·¸ì™€ ê´€ë ¨ëœ íƒœê·¸ë“¤ ë°˜í™˜ (ê³ ê¸‰ ë¶„ì„)"""
    try:
        webtoons_data = load_webtoon_data()
        
        # íƒœê·¸ ì •ê·œí™”
        normalized_tag = normalize_tag(tag)
        
        related_tags = get_related_tags_advanced(normalized_tag, webtoons_data, limit)
        
        return {
            "success": True,
            "data": {
                "target_tag": normalized_tag,
                "related_tags": related_tags,
                "count": len(related_tags),
                "analysis_method": "weighted_correlation"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê´€ë ¨ íƒœê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/stats")
async def get_statistics():
    """ì „ì²´ í†µê³„ ë°˜í™˜"""
    try:
        webtoons_data = load_webtoon_data()
        
        total_webtoons = len(webtoons_data)
        avg_rating = np.mean([w['rating'] for w in webtoons_data])
        avg_interest = np.mean([w['interest_count'] for w in webtoons_data])
        
        all_tags = []
        for w in webtoons_data:
            all_tags.extend(w['tags'])
        unique_tags = len(set(all_tags))
        
        return {
            "success": True,
            "data": {
                "total_webtoons": total_webtoons,
                "avg_rating": round(avg_rating, 2),
                "avg_interest": int(avg_interest),
                "unique_tags": unique_tags,
                "tfidf_features": len(tfidf_analyzer.feature_names) if tfidf_analyzer.feature_names is not None else 0,
                "analysis_enhanced": tfidf_analyzer.tfidf_matrix is not None,
                "last_updated": datetime.now().isoformat()
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

def generate_heatmap_data(webtoons_data):
    """ížˆíŠ¸ë§µ ë°ì´í„° ìƒì„± (í•œêµ­ì–´ ìž¥ë¥´)"""
    genres = ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ì¼ìƒ','ë¬´í˜‘/ì‚¬ê·¹','ìŠ¤ë¦´ëŸ¬']
    demographics = ['ë‚¨ì„±-10ëŒ€', 'ë‚¨ì„±-20ëŒ€', 'ë‚¨ì„±-30ëŒ€', 'ì—¬ì„±-10ëŒ€', 'ì—¬ì„±-20ëŒ€', 'ì—¬ì„±-30ëŒ€']
    
    heatmap_data = []
    
    for demo_idx, demo in enumerate(demographics):
        gender, age = demo.split('-')
        for genre_idx, genre in enumerate(genres):
            count = sum(1 for w in webtoons_data 
                       if w['gender'] == gender and w['ages'] == age and genre in w['tags'])
            
            # í‰ê·  í‰ì ë„ ê³„ì‚°
            genre_webtoons = [w for w in webtoons_data 
                            if w['gender'] == gender and w['ages'] == age and genre in w['tags']]
            avg_rating = np.mean([w['rating'] for w in genre_webtoons]) if genre_webtoons else 0
            
            heatmap_data.append({
                'x': genre_idx,
                'y': demo_idx,
                'value': count,
                'genre': genre,
                'demographic': demo,
                'count': count,
                'avg_rating': round(avg_rating, 2),
                'intensity': count / max(1, max([sum(1 for w in webtoons_data if g in w['tags']) for g in genres]))
            })
    
    return heatmap_data

@app.get("/webtoon-api/api/analysis/heatmap")
async def get_heatmap_analysis():
    """ížˆíŠ¸ë§µ ë¶„ì„ ë°ì´í„° ë°˜í™˜ (í•œêµ­ì–´ ê°œì„ )"""
    try:
        webtoons_data = load_webtoon_data()
        heatmap_data = generate_heatmap_data(webtoons_data)
        
        return {
            "success": True, 
            "data": heatmap_data,
            "metadata": {
                "genres": ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ì¼ìƒ', 'ìŠ¤ë¦´ëŸ¬'],
                "demographics": ['ë‚¨ì„±-10ëŒ€', 'ë‚¨ì„±-20ëŒ€', 'ë‚¨ì„±-30ëŒ€', 'ì—¬ì„±-10ëŒ€', 'ì—¬ì„±-20ëŒ€', 'ì—¬ì„±-30ëŒ€'],
                "total_combinations": len(heatmap_data)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ížˆíŠ¸ë§µ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
def create_tag_matrix(webtoons_data, min_count=3):
    """íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± (Python ë¶„ì„ ì½”ë“œ ê¸°ë°˜)"""
    print("ðŸ·ï¸ íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì¤‘...")
    
    # ëª¨ë“  íƒœê·¸ ìˆ˜ì§‘ ë° ë¹ˆë„ ê³„ì‚°
    all_tags = []
    for webtoon in webtoons_data:
        if isinstance(webtoon['tags'], list):
            all_tags.extend(webtoon['tags'])
    
    tag_counts = Counter(all_tags)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒì¸ íƒœê·¸ë§Œ ì„ íƒ
    frequent_tags = [tag for tag, count in tag_counts.items() if count >= min_count]
    frequent_tags = sorted(frequent_tags, key=lambda x: tag_counts[x], reverse=True)
    
    print(f"ðŸ“Š ì´ ê³ ìœ  íƒœê·¸: {len(tag_counts)}ê°œ")
    print(f"ðŸ“Š ë¶„ì„ ëŒ€ìƒ íƒœê·¸ ({min_count}íšŒ ì´ìƒ): {len(frequent_tags)}ê°œ")
    
    # íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    tag_matrix = np.zeros((len(frequent_tags), len(frequent_tags)))
    tag_to_idx = {tag: idx for idx, tag in enumerate(frequent_tags)}
    
    for webtoon in webtoons_data:
        current_tags = [tag for tag in webtoon['tags'] if tag in tag_to_idx]
        
        # ë™ì‹œ ì¶œí˜„ ê¸°ë¡ (ê°€ì¤‘ì¹˜ ì ìš©)
        weight = (webtoon['rating'] / 10.0) * (1 + np.log10(webtoon['interest_count'] + 1) / 10)
        
        for tag in current_tags:
            tag_matrix[tag_to_idx[tag], tag_to_idx[tag]] += weight
        
        for tag1, tag2 in combinations(current_tags, 2):
            idx1, idx2 = tag_to_idx[tag1], tag_to_idx[tag2]
            tag_matrix[idx1, idx2] += weight
            tag_matrix[idx2, idx1] += weight
    
    return tag_matrix, frequent_tags, tag_counts

def calculate_tag_correlations(tag_matrix, frequent_tags):
    """íƒœê·¸ ê°„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (Python ë¶„ì„ ì½”ë“œ ê¸°ë°˜)"""
    print("ðŸ”— íƒœê·¸ ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...")
    
    # ëŒ€ê°ì„  ìš”ì†Œë¥¼ 0ìœ¼ë¡œ ì„¤ì •
    np.fill_diagonal(tag_matrix, 0)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    correlation_matrix = cosine_similarity(tag_matrix)
    
    # ìƒê´€ê´€ê³„ê°€ ë†’ì€ íƒœê·¸ ìŒ ì°¾ê¸°
    correlations = []
    n_tags = len(frequent_tags)
    
    for i in range(n_tags):
        for j in range(i+1, n_tags):
            if correlation_matrix[i, j] > 0:
                correlations.append({
                    'tag1': frequent_tags[i],
                    'tag2': frequent_tags[j],
                    'correlation': correlation_matrix[i, j],
                    'co_occurrence': tag_matrix[i, j]
                })
    
    correlations = sorted(correlations, key=lambda x: x['correlation'], reverse=True)
    return correlation_matrix, correlations

def create_advanced_network_data(webtoons_data, selected_tags=None, min_correlation=0.2, max_nodes=30):
    """ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± (í•œêµ­ì–´ ê¸°ë°˜)"""
    print("ðŸ•¸ï¸ ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ê·¸ëž˜í”„ ìƒì„± ì¤‘...")
    
    # íƒœê·¸ ë§¤íŠ¸ë¦­ìŠ¤ ë° ìƒê´€ê´€ê³„ ê³„ì‚°
    tag_matrix, frequent_tags, tag_counts = create_tag_matrix(webtoons_data)
    _, correlations = calculate_tag_correlations(tag_matrix, frequent_tags)
    
    # ì „ì²´ íƒœê·¸ ì˜í–¥ë ¥ ê³„ì‚° (ëª¨ë“  ê²½ìš°ì— í•„ìš”)
    tag_influence = defaultdict(float)
    for tag in frequent_tags[:50]:
        frequency_score = tag_counts.get(tag, 0) / max(tag_counts.values())
        
        connection_score = 0
        for corr in correlations[:50]:
            if corr['tag1'] == tag or corr['tag2'] == tag:
                connection_score += corr['correlation']
        
        tag_influence[tag] = frequency_score * 0.6 + (connection_score / 10) * 0.4

    # ì„ íƒëœ íƒœê·¸ê°€ ìžˆìœ¼ë©´ í•´ë‹¹ íƒœê·¸ì™€ ì—°ê²°ëœ ë…¸ë“œë“¤ë§Œ í¬í•¨
    if selected_tags:
        relevant_tags = set(selected_tags)
        for corr in correlations:
            if corr['correlation'] >= min_correlation:
                if corr['tag1'] in selected_tags or corr['tag2'] in selected_tags:
                    relevant_tags.add(corr['tag1'])
                    relevant_tags.add(corr['tag2'])
        
        # ì—°ê²°ì„±ì´ ë†’ì€ íƒœê·¸ë“¤ ìš°ì„  ì„ íƒ
        tag_connections = defaultdict(float)
        for corr in correlations[:100]:
            if corr['correlation'] >= min_correlation:
                tag_connections[corr['tag1']] += corr['correlation']
                tag_connections[corr['tag2']] += corr['correlation']
        
        sorted_tags = sorted(
            [(tag, score) for tag, score in tag_connections.items() if tag in relevant_tags],
            key=lambda x: x[1], reverse=True
        )
        top_tags = set([tag for tag, _ in sorted_tags[:max_nodes]])
    else:
        # ì „ì²´ íƒœê·¸ì—ì„œ ìƒìœ„ ë…¸ë“œ ì„ íƒ
        sorted_tags = sorted(tag_influence.items(), key=lambda x: x[1], reverse=True)
        top_tags = set([tag for tag, _ in sorted_tags[:max_nodes]])
    
    # ë…¸ë“œ ìƒì„± (í•œêµ­ì–´ ì¹´í…Œê³ ë¦¬)
    nodes = []
    for tag in top_tags:
        count = tag_counts.get(tag, 0)
        
        # íƒœê·¸ë³„ ì›¹íˆ°ì˜ í‰ê·  í‰ì ê³¼ ì¡°íšŒìˆ˜ ê³„ì‚°
        tag_webtoons = [w for w in webtoons_data if tag in w['tags']]
        avg_rating = np.mean([w['rating'] for w in tag_webtoons]) if tag_webtoons else 0
        avg_interest = np.mean([w['interest_count'] for w in tag_webtoons]) if tag_webtoons else 0
        
        # ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
        influence = tag_influence.get(tag, 0)
        
        nodes.append({
            'id': tag,
            'count': count,
            'influence': round(influence, 3),
            'avg_rating': round(avg_rating, 2),
            'avg_interest': int(avg_interest),
            'size': min(max(count * 2, 15), 60),
            'group': get_korean_tag_category(tag),
            'selected': tag in (selected_tags or [])
        })
    
    # ë§í¬ ìƒì„±
    links = []
    node_ids = set(node['id'] for node in nodes)
    
    for corr in correlations:
        if (corr['correlation'] >= min_correlation and 
            corr['tag1'] in node_ids and corr['tag2'] in node_ids):
            
            links.append({
                'source': corr['tag1'],
                'target': corr['tag2'],
                'value': round(corr['correlation'], 3),
                'co_occurrence': round(corr['co_occurrence'], 1),
                'width': min(max(corr['correlation'] * 10, 1), 8)
            })
    
    # ìƒê´€ê´€ê³„ ìˆœìœ¼ë¡œ ì •ë ¬
    links.sort(key=lambda x: x['value'], reverse=True)
    
    return {
        'nodes': nodes,
        'links': links[:100],  # ìƒìœ„ 100ê°œ ë§í¬ë§Œ
        'summary': {
            'total_nodes': len(nodes),
            'total_links': len(links),
            'selected_tags': selected_tags or [],
            'max_correlation': max([l['value'] for l in links]) if links else 0,
            'avg_correlation': np.mean([l['value'] for l in links]) if links else 0
        },
        'analysis_stats': {
            'total_unique_tags': len(tag_counts),
            'frequent_tags_count': len(frequent_tags),
            'correlation_threshold': min_correlation
        }
    }

def get_korean_tag_category(tag):
    """í•œêµ­ì–´ íƒœê·¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    categories = {
        'ìž¥ë¥´': ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ìŠ¤ë¦´ëŸ¬', 'í˜¸ëŸ¬', 'ì½”ë¯¸ë””', 'ì¼ìƒ', 'ë¬´í˜‘'],
        'í…Œë§ˆ': ['íšŒê·€', 'ì„±ìž¥', 'ë³µìˆ˜', 'í•™ì›', 'í˜„ì‹¤', 'ê²Œìž„', 'ëª¨í—˜', 'ìš”ë¦¬', 'ìŠ¤í¬ì¸ '],
        'ìŠ¤íƒ€ì¼': ['ëª…ìž‘', 'ë‹¨íŽ¸', 'ëŸ¬ë¸”ë¦¬'],
        'ì„¤ì •': ['ì„œì–‘', 'ê·€ì¡±', 'í˜„ëŒ€', 'ë¯¸ëž˜', 'ê³¼ê±°', 'ë†êµ¬']
    }
    
    for category, tags in categories.items():
        if any(keyword in tag for keyword in tags):
            return category
    
    return 'ê¸°íƒ€'

def get_related_tags_advanced(target_tag, webtoons_data, limit=10):
    """íŠ¹ì • íƒœê·¸ì™€ ê´€ë ¨ëœ íƒœê·¸ë“¤ ì°¾ê¸° (ê³ ê¸‰ ë²„ì „)"""
    print(f"ðŸ” '{target_tag}' íƒœê·¸ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...")
    
    related_scores = defaultdict(float)
    target_webtoons = [w for w in webtoons_data if target_tag in w['tags']]
    
    for webtoon in target_webtoons:
        # ê°€ì¤‘ì¹˜: í‰ì ê³¼ ì¡°íšŒìˆ˜ë¥¼ ê³ ë ¤
        weight = (webtoon['rating'] / 10.0) * (1 + np.log10(webtoon['interest_count'] + 1) / 20)
        
        for tag in webtoon['tags']:
            if tag != target_tag:
                related_scores[tag] += weight
    
    # ì •ê·œí™”
    if related_scores:
        max_score = max(related_scores.values())
        for tag in related_scores:
            related_scores[tag] = related_scores[tag] / max_score
    
    sorted_related = sorted(related_scores.items(), key=lambda x: x[1], reverse=True)
    
    return [{
        'tag': tag,
        'score': round(score, 3),
        'count': sum(1 for w in target_webtoons if tag in w['tags']),
        'category': get_korean_tag_category(tag)
    } for tag, score in sorted_related[:limit]]

def analyze_tag_connectivity(webtoons_data, min_correlation=0.15):
    """íƒœê·¸ë³„ ì—°ê²°ì„± ë¶„ì„ - ê° íƒœê·¸ê°€ ëª‡ ê°œì˜ ë‹¤ë¥¸ íƒœê·¸ì™€ ì—°ê²°ë˜ì–´ ìžˆëŠ”ì§€ ë¶„ì„"""
    print("ðŸ•¸ï¸ íƒœê·¸ ì—°ê²°ì„± ë¶„ì„ ì‹œìž‘...")
    
    # íƒœê·¸ ë§¤íŠ¸ë¦­ìŠ¤ ë° ìƒê´€ê´€ê³„ ê³„ì‚°
    tag_matrix, frequent_tags, tag_counts = create_tag_matrix(webtoons_data)
    _, correlations = calculate_tag_correlations(tag_matrix, frequent_tags)
    
    # ê° íƒœê·¸ë³„ ì—°ê²°ëœ íƒœê·¸ë“¤ê³¼ ì—°ê²° ê°•ë„ ê³„ì‚°
    tag_connections = {}
    
    for tag in frequent_tags:
        connected_tags = []
        
        # ì´ íƒœê·¸ì™€ ì—°ê²°ëœ ëª¨ë“  íƒœê·¸ë“¤ ì°¾ê¸°
        for corr in correlations:
            if corr['correlation'] >= min_correlation:
                if corr['tag1'] == tag:
                    connected_tags.append({
                        'connected_tag': corr['tag2'],
                        'correlation': round(corr['correlation'], 3),
                        'co_occurrence': round(corr['co_occurrence'], 1)
                    })
                elif corr['tag2'] == tag:
                    connected_tags.append({
                        'connected_tag': corr['tag1'], 
                        'correlation': round(corr['correlation'], 3),
                        'co_occurrence': round(corr['co_occurrence'], 1)
                    })
        
        # ì—°ê²° ê°•ë„ìˆœìœ¼ë¡œ ì •ë ¬
        connected_tags.sort(key=lambda x: x['correlation'], reverse=True)
        
        tag_connections[tag] = {
            'tag': tag,
            'connection_count': len(connected_tags),
            'connected_tags': connected_tags,
            'frequency': tag_counts.get(tag, 0),
            'avg_correlation': round(np.mean([ct['correlation'] for ct in connected_tags]), 3) if connected_tags else 0,
            'category': get_korean_tag_category(tag)
        }
    
    # ì—°ê²°ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_connectivity = sorted(
        tag_connections.values(),
        key=lambda x: (x['connection_count'], x['avg_correlation']),
        reverse=True
    )
    
    return sorted_connectivity

@app.get("/webtoon-api/api/analysis/tag-connectivity")
async def get_tag_connectivity(
    min_correlation: Optional[float] = Query(0.15, description="ìµœì†Œ ìƒê´€ê³„ìˆ˜"),
    top_n: Optional[int] = Query(15, description="ìƒìœ„ Nê°œ íƒœê·¸")
):
    """íƒœê·¸ë³„ ì—°ê²°ì„± ë¶„ì„ - ê° íƒœê·¸ê°€ ëª‡ ê°œì˜ ë‹¤ë¥¸ íƒœê·¸ì™€ ì—°ê²°ë˜ì–´ ìžˆëŠ”ì§€"""
    try:
        webtoons_data = load_webtoon_data()
        connectivity_data = analyze_tag_connectivity(webtoons_data, min_correlation)
        
        # ìƒìœ„ Nê°œë§Œ ì„ íƒ
        top_connectivity = connectivity_data[:top_n]
        
        # ìš”ì•½ í†µê³„
        summary = {
            "total_analyzed_tags": len(connectivity_data),
            "min_correlation_threshold": min_correlation,
            "most_connected_tag": connectivity_data[0]['tag'] if connectivity_data else None,
            "max_connections": connectivity_data[0]['connection_count'] if connectivity_data else 0,
            "avg_connections": round(np.mean([t['connection_count'] for t in connectivity_data]), 1) if connectivity_data else 0
        }
        
        return {
            "success": True,
            "data": {
                "top_connected_tags": top_connectivity,
                "summary": summary,
                "analysis_info": {
                    "description": "ê° íƒœê·¸ê°€ ë‹¤ë¥¸ íƒœê·¸ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ìžˆëŠ”ì§€ ë¶„ì„",
                    "correlation_method": "cosine_similarity",
                    "weight_factors": ["rating", "interest_count"],
                    "min_tag_frequency": 3
                }
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒœê·¸ ì—°ê²°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/webtoon-api/api/analysis/insights")
async def get_insights():
    """ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
    try:
        webtoons_data = load_webtoon_data()
        
        # íƒœê·¸ íŠ¸ë Œë“œ ë¶„ì„
        all_tags = []
        for w in webtoons_data:
            all_tags.extend(w['tags'])
        tag_frequency = Counter(all_tags)
        
        # ì„±ë³„ë³„ ì„ í˜¸ íƒœê·¸
        male_tags = []
        female_tags = []
        for w in webtoons_data:
            if w['gender'] == 'ë‚¨ì„±':
                male_tags.extend(w['tags'])
            else:
                female_tags.extend(w['tags'])
        
        male_preferences = Counter(male_tags).most_common(10)
        female_preferences = Counter(female_tags).most_common(10)
        
        # ê³ í‰ì  íƒœê·¸ ë¶„ì„
        high_rated_tags = defaultdict(list)
        for w in webtoons_data:
            if w['rating'] >= 9.5:
                for tag in w['tags']:
                    high_rated_tags[tag].append(w['rating'])
        
        quality_tags = {
            tag: round(np.mean(ratings), 2) 
            for tag, ratings in high_rated_tags.items() 
            if len(ratings) >= 3
        }
        quality_tags = sorted(quality_tags.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "success": True,
            "data": {
                "trending_tags": tag_frequency.most_common(10),
                "male_preferences": male_preferences,
                "female_preferences": female_preferences,
                "quality_indicators": quality_tags,
                "insights": {
                    "most_popular_genre": tag_frequency.most_common(1)[0][0],
                    "gender_difference": len(set(dict(male_preferences[:5]).keys()) - set(dict(female_preferences[:5]).keys())),
                    "quality_vs_popularity": "í‰ì ê³¼ ì¸ê¸°ë„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼"
                }
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@app.get("/webtoon-api/api/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0",
        "features": [
            "tfidf_analysis", 
            "korean_nlp", 
            "hybrid_recommendations", 
            "keyword_extraction",
            "heatmap_analysis"
        ],
        "tfidf_ready": tfidf_analyzer.tfidf_matrix is not None,
        "konlpy_available": KONLPY_AVAILABLE
    }

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("enhanced_main:app", host="0.0.0.0", port=port, reload=False)